{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dd7744-0cd7-46a3-a47f-f530f9dbe798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device: NVIDIA RTX A5000\n",
      "Using device: cuda\n",
      "Languages: ['Bengali', 'Gujarati', 'Hindi', 'Kannada', 'Malayalam', 'Marathi', 'Punjabi', 'Tamil', 'Telugu', 'Urdu']\n",
      "Total languages: 10\n",
      "Current working directory: /home/teaching\n",
      "Checking if MP3_DIR exists: True\n",
      "Contents of MP3_DIR: ['Kannada', 'Marathi', 'Punjabi', 'Telugu', 'Gujarati', 'Malayalam', 'Urdu', 'Tamil', 'Hindi', 'Bengali']\n",
      "Starting data preprocessing...\n",
      "Skipping MP3 to WAV conversion, found 256826 existing WAV files\n",
      "Creating data splits...\n",
      "Total samples: 256826\n",
      "lang\n",
      "Urdu         31959\n",
      "Bengali      27258\n",
      "Gujarati     26439\n",
      "Punjabi      26227\n",
      "Hindi        25462\n",
      "Marathi      25378\n",
      "Tamil        24195\n",
      "Malayalam    24044\n",
      "Telugu       23656\n",
      "Kannada      22208\n",
      "Name: count, dtype: int64\n",
      "Splits created and saved to G44/splits:\n",
      "  - Train: 205457 samples\n",
      "  - Validation: 25676 samples\n",
      "  - Test: 25693 samples\n",
      "Starting model training...\n",
      "Using device: cuda\n",
      "Model: Whisper-tiny for LID\n",
      "Trainable parameters: 101,130 / 7,733,514\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|████▌                                                      | 1000/12842 [02:56<44:19,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000: loss=2.1901, acc=22.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█████████▏                                                 | 1999/12842 [05:52<32:06,  5.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000: loss=2.0502, acc=29.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|█████████████▊                                             | 2999/12842 [08:45<29:40,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3000: loss=1.9200, acc=35.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|██████████████████▍                                        | 4000/12842 [11:40<26:51,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000: loss=1.8078, acc=39.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|██████████████████████▉                                    | 4997/12842 [14:31<22:37,  5.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5000: loss=1.7161, acc=42.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|███████████████████████████▌                               | 6000/12842 [17:27<19:36,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6000: loss=1.6390, acc=44.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|████████████████████████████████▏                          | 7000/12842 [20:21<17:06,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7000: loss=1.5741, acc=46.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|████████████████████████████████████▋                      | 7999/12842 [23:12<13:23,  6.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8000: loss=1.5173, acc=48.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|█████████████████████████████████████████▎                 | 8997/12842 [26:05<11:14,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9000: loss=1.4694, acc=49.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████████████████████████████████████████▉               | 9568/12842 [27:44<09:16,  5.88it/s]"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 1. SETUP AND IMPORTS\n",
    "# ================================================================\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import whisper\n",
    "import librosa\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add this helper function for progress bars\n",
    "def safe_tqdm(iterable, desc=None, **kwargs):\n",
    "    \"\"\"A tqdm wrapper that doesn't break in various environments\"\"\"\n",
    "    try:\n",
    "        # Try to use regular tqdm with minimal features\n",
    "        return tqdm(iterable, desc=desc, miniters=1, mininterval=0.5, **kwargs)\n",
    "    except Exception as e:\n",
    "        # If all else fails, return the iterable without progress tracking\n",
    "        print(f\"Progress bar disabled: {str(e)}\")\n",
    "        return iterable\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"PySoundFile failed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"n_fft=.*is too large for input signal\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"librosa.core.audio.__audioread_load\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"frames must be specified for non-seekable files\")\n",
    "\n",
    "# Check CUDA\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "# Set device to GPU if available, else CPU\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Set paths for the dataset\n",
    "MP3_DIR = \"G44/data/mp3s/Language Detection Dataset\"  # Path to MP3 files\n",
    "WAV_DIR = \"G44/wavs\"                                 # Output path for WAV files\n",
    "SPLITS_DIR = \"G44/splits\"                            # Output path for data splits\n",
    "OUTPUT_DIR = \"G44/models\"                            # Output path for models\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(WAV_DIR, exist_ok=True)\n",
    "os.makedirs(SPLITS_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ================================================================\n",
    "# 2. LANGUAGES AND MAPPINGS\n",
    "# ================================================================\n",
    "\n",
    "# List of Indian languages for the project (exact casing as in directory)\n",
    "LANGS = [\n",
    "    'Bengali', 'Gujarati', 'Hindi', 'Kannada', 'Malayalam',\n",
    "    'Marathi', 'Punjabi', 'Tamil', 'Telugu', 'Urdu'\n",
    "]\n",
    "\n",
    "# Create a mapping from language to ID (use lowercase for internal processing)\n",
    "LANG2ID = {lang.lower(): idx for idx, lang in enumerate(LANGS)}\n",
    "ID2LANG = {idx: lang.lower() for idx, lang in enumerate(LANGS)}\n",
    "\n",
    "print(\"Languages:\", LANGS)\n",
    "print(\"Total languages:\", len(LANGS))\n",
    "\n",
    "# ================================================================\n",
    "# 3. AUDIO AUGMENTATION CLASSES\n",
    "# ================================================================\n",
    "\n",
    "class RandomApply(nn.Module):\n",
    "    \"\"\"Randomly apply a list of transformations with a given probability\"\"\"\n",
    "    def __init__(self, transforms, p=0.5):\n",
    "        super().__init__()\n",
    "        self.transforms = transforms\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.p < torch.rand(1):\n",
    "            return x\n",
    "        for t in self.transforms:\n",
    "            x = t(x)\n",
    "        return x\n",
    "\n",
    "class AddBackgroundNoise(nn.Module):\n",
    "    \"\"\"Add background noise to audio at a given SNR level\"\"\"\n",
    "    def __init__(self, snr_db_range=(5, 20)):\n",
    "        super().__init__()\n",
    "        self.snr_db_range = snr_db_range\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Generate white noise\n",
    "        noise = torch.randn_like(x)\n",
    "        \n",
    "        # Calculate signal and noise power\n",
    "        signal_power = torch.mean(x ** 2)\n",
    "        noise_power = torch.mean(noise ** 2)\n",
    "        \n",
    "        # Random SNR from range\n",
    "        snr_db = torch.tensor(\n",
    "            np.random.uniform(*self.snr_db_range)\n",
    "        )\n",
    "        \n",
    "        # Calculate noise scaling factor\n",
    "        snr = 10 ** (snr_db / 10)\n",
    "        scale = torch.sqrt(signal_power / (noise_power * snr))\n",
    "        \n",
    "        # Add scaled noise\n",
    "        return x + scale * noise\n",
    "\n",
    "class SpeedPerturb(nn.Module):\n",
    "    \"\"\"Apply speed perturbation to audio (GPU compatible)\"\"\"\n",
    "    def __init__(self, speed_range=(0.9, 1.1), sample_rate=16000):\n",
    "        super().__init__()\n",
    "        self.speed_range = speed_range\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Random speed factor\n",
    "        speed_factor = np.random.uniform(*self.speed_range)\n",
    "        \n",
    "        # Convert tensor to numpy if needed\n",
    "        x_np = x.cpu().numpy() if isinstance(x, torch.Tensor) else x\n",
    "        \n",
    "        # Use librosa for time stretching\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            y = librosa.effects.time_stretch(x_np, rate=1.0/speed_factor)\n",
    "        \n",
    "        # Convert back to tensor\n",
    "        y = torch.tensor(y, dtype=torch.float32, device=x.device if isinstance(x, torch.Tensor) else None)\n",
    "        \n",
    "        # Handle length changes\n",
    "        if len(y) > len(x):\n",
    "            y = y[:len(x)]\n",
    "        elif len(y) < len(x):\n",
    "            # Pad with zeros\n",
    "            padding = torch.zeros(len(x) - len(y), device=y.device)\n",
    "            y = torch.cat([y, padding])\n",
    "            \n",
    "        return y\n",
    "\n",
    "class PitchShift(nn.Module):\n",
    "    \"\"\"Apply pitch shifting to audio (GPU compatible)\"\"\"\n",
    "    def __init__(self, pitch_range=(-2, 2), sample_rate=16000):\n",
    "        super().__init__()\n",
    "        self.pitch_range = pitch_range\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Random pitch shift in semitones\n",
    "        n_semitones = np.random.uniform(*self.pitch_range)\n",
    "        \n",
    "        # Convert tensor to numpy if needed\n",
    "        x_np = x.cpu().numpy() if isinstance(x, torch.Tensor) else x\n",
    "        \n",
    "        # Use librosa for pitch shifting\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            y = librosa.effects.pitch_shift(\n",
    "                x_np, \n",
    "                sr=self.sample_rate,\n",
    "                n_steps=n_semitones\n",
    "            )\n",
    "        \n",
    "        # Convert back to tensor\n",
    "        y = torch.tensor(y, dtype=torch.float32, device=x.device if isinstance(x, torch.Tensor) else None)\n",
    "        \n",
    "        # Handle length changes\n",
    "        if len(y) > len(x):\n",
    "            y = y[:len(x)]\n",
    "        elif len(y) < len(x):\n",
    "            padding = torch.zeros(len(x) - len(y), device=y.device)\n",
    "            y = torch.cat([y, padding])\n",
    "            \n",
    "        return y\n",
    "\n",
    "# ================================================================\n",
    "# 4. DATASET IMPLEMENTATION\n",
    "# ================================================================\n",
    "\n",
    "class WhisperFeatureDataset(Dataset):\n",
    "    \"\"\"Dataset for extracting features from audio files using Whisper model\"\"\"\n",
    "    def __init__(self, csv_path, augment=False, max_duration=30):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path: Path to CSV file with 'path' and 'lang' columns\n",
    "            augment: Whether to apply augmentation to audio\n",
    "            max_duration: Maximum audio duration in seconds (for padding/truncation)\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.paths = df['path'].tolist()\n",
    "        self.labels = [LANG2ID[l.lower()] for l in df['lang']]\n",
    "        self.augment = augment\n",
    "        self.max_duration = max_duration\n",
    "        \n",
    "        # Use Whisper's feature extraction\n",
    "        self.mel_fn = whisper.audio.log_mel_spectrogram\n",
    "        \n",
    "        # Audio augmentations\n",
    "        self.transforms = nn.Sequential(\n",
    "            RandomApply([AddBackgroundNoise()], p=0.5),\n",
    "            RandomApply([SpeedPerturb()], p=0.4),\n",
    "            RandomApply([PitchShift()], p=0.3),\n",
    "        )\n",
    "        \n",
    "    def load_audio(self, path):\n",
    "        \"\"\"Robust audio loading function with updated methods\"\"\"\n",
    "        # Suppress warnings\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            \n",
    "            try:\n",
    "                # First try soundfile (most efficient for WAV)\n",
    "                with sf.SoundFile(path) as sound_file:\n",
    "                    wav = sound_file.read(dtype='float32')\n",
    "                    sr = sound_file.samplerate\n",
    "                    \n",
    "                    # Convert to mono if stereo\n",
    "                    if wav.ndim > 1:\n",
    "                        wav = wav.mean(axis=1)\n",
    "            except Exception as e:\n",
    "                # Try torchaudio next\n",
    "                try:\n",
    "                    # Try to set best backend\n",
    "                    try:\n",
    "                        torchaudio.set_audio_backend(\"sox_io\")  # or \"soundfile\"\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    waveform, sr = torchaudio.load(path)\n",
    "                    if waveform.shape[0] > 1:  # Multi-channel, convert to mono\n",
    "                        wav = waveform.mean(dim=0).numpy()\n",
    "                    else:\n",
    "                        wav = waveform.squeeze(0).numpy()\n",
    "                except Exception as e2:\n",
    "                    # Fall back to librosa with explicit parameters\n",
    "                    try:\n",
    "                        wav, sr = librosa.load(\n",
    "                            path,\n",
    "                            sr=None,\n",
    "                            mono=True,\n",
    "                            offset=0.0,\n",
    "                            duration=None,\n",
    "                            dtype=np.float32,\n",
    "                            res_type='kaiser_best'\n",
    "                        )\n",
    "                    except Exception as e3:\n",
    "                        print(f\"Failed to load {path}: {e3}\")\n",
    "                        # Return empty audio as fallback\n",
    "                        sr = 16000\n",
    "                        wav = np.zeros(sr)\n",
    "                \n",
    "        return torch.tensor(wav).float(), sr\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load audio file with robust loader\n",
    "        wav, sr = self.load_audio(self.paths[idx])\n",
    "        \n",
    "        # Apply augmentations if needed\n",
    "        if self.augment:\n",
    "            wav = self.transforms(wav)\n",
    "        \n",
    "        # Resample if needed\n",
    "        if sr != 16000:\n",
    "            wav = torchaudio.functional.resample(wav, orig_freq=sr, new_freq=16000)\n",
    "        \n",
    "        # Pad or trim to expected length (whisper expects 30 s max)\n",
    "        wav = whisper.audio.pad_or_trim(wav, length=16000 * self.max_duration)\n",
    "        \n",
    "        # Extract mel spectrogram features\n",
    "        mel = self.mel_fn(wav)\n",
    "        \n",
    "        return mel, self.labels[idx]\n",
    "\n",
    "def get_dataloaders(train_csv, val_csv, test_csv=None, batch_size=32, num_workers=4):\n",
    "    \"\"\"Create dataloaders for training, validation, and testing\"\"\"\n",
    "    # Create datasets\n",
    "    train_ds = WhisperFeatureDataset(train_csv, augment=True)\n",
    "    val_ds = WhisperFeatureDataset(val_csv, augment=False)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=num_workers, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_ds, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    # Create test dataloader if test CSV is provided\n",
    "    test_loader = None\n",
    "    if test_csv:\n",
    "        test_ds = WhisperFeatureDataset(test_csv, augment=False)\n",
    "        test_loader = DataLoader(\n",
    "            test_ds, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# ================================================================\n",
    "# 5. MODEL ARCHITECTURE\n",
    "# ================================================================\n",
    "\n",
    "class LIDWhisper(nn.Module):\n",
    "    \"\"\"\n",
    "    Whisper model adapted for language identification\n",
    "    Uses the encoder from Whisper and replaces the decoder with a classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, whisper_model_name='base', num_languages=10, freeze_encoder=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            whisper_model_name: Whisper model size ('tiny', 'base', 'small', etc.)\n",
    "            num_languages: Number of languages to classify\n",
    "            freeze_encoder: Whether to freeze the Whisper encoder weights\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load the pretrained Whisper model\n",
    "        whisper_model = whisper.load_model(whisper_model_name)\n",
    "        \n",
    "        # Extract the encoder\n",
    "        self.encoder = whisper_model.encoder\n",
    "        \n",
    "        # Get model dimensions\n",
    "        d_model = whisper_model.dims.n_audio_state\n",
    "        \n",
    "        # Freeze encoder weights if specified\n",
    "        if freeze_encoder:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad_(False)\n",
    "        \n",
    "        # Replace decoder with a classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_languages)\n",
    "        )\n",
    "    \n",
    "    def forward(self, mel):\n",
    "        # Pass through encoder\n",
    "        encoder_out = self.encoder(mel)\n",
    "        \n",
    "        # Mean pooling over time dimension\n",
    "        pooled = encoder_out.mean(dim=1)\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "def create_model(whisper_size='base', num_languages=10, freeze_encoder=True, device='cuda'):\n",
    "    \"\"\"\n",
    "    Create a Whisper-based language identification model\n",
    "    \n",
    "    Args:\n",
    "        whisper_size: Whisper model size ('tiny', 'base', 'small', etc.)\n",
    "        num_languages: Number of languages to classify\n",
    "        freeze_encoder: Whether to freeze the encoder weights\n",
    "        device: Device to load the model onto\n",
    "        \n",
    "    Returns:\n",
    "        model: LIDWhisper model\n",
    "    \"\"\"\n",
    "    model = LIDWhisper(whisper_size, num_languages, freeze_encoder)\n",
    "    return model.to(device)\n",
    "\n",
    "# ================================================================\n",
    "# 6. TRAINING AND EVALUATION FUNCTIONS\n",
    "# ================================================================\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train the model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Training loop\n",
    "    progress_bar = safe_tqdm(dataloader, desc=\"Training\")\n",
    "    batch_count = 0\n",
    "    for mel, labels in progress_bar:\n",
    "        # Move data to device\n",
    "        mel, labels = mel.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(mel)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        batch_count += 1\n",
    "        if batch_count % 1000 == 0:\n",
    "            print(f\"Batch {batch_count}: loss={total_loss/batch_count:.4f}, acc={100*correct/total:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        \"loss\": total_loss / len(dataloader),\n",
    "        \"accuracy\": correct / total\n",
    "    }\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate the model on validation/test set\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for mel, labels in safe_tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            # Move data to device\n",
    "            mel, labels = mel.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(mel)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Collect predictions and labels\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Update loss\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    \n",
    "    return {\n",
    "        \"loss\": total_loss / len(dataloader),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"preds\": all_preds,\n",
    "        \"labels\": all_labels\n",
    "    }\n",
    "\n",
    "def save_model(model, path):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "    \n",
    "def load_model(model, path, device):\n",
    "    \"\"\"Load model checkpoint\"\"\"\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return model\n",
    "\n",
    "# ================================================================\n",
    "# 7. DATA PREPROCESSING FUNCTIONS\n",
    "# ================================================================\n",
    "\n",
    "def convert_mp3_to_wav(mp3_path, wav_path, sr=16000):\n",
    "    \"\"\"Convert MP3 file to WAV with specified sample rate - FIXED VERSION\"\"\"\n",
    "    try:\n",
    "        # Try different loading methods in sequence\n",
    "        audio, orig_sr = None, None\n",
    "        \n",
    "        # Suppress warnings\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            \n",
    "            # Try direct ffmpeg approach first for MP3 files\n",
    "            if mp3_path.lower().endswith('.mp3'):\n",
    "                try:\n",
    "                    # Use torchaudio with ffmpeg backend for MP3\n",
    "                    try:\n",
    "                        torchaudio.set_audio_backend(\"sox_io\")  # or \"soundfile\" depending on availability\n",
    "                    except:\n",
    "                        pass\n",
    "                        \n",
    "                    waveform, orig_sr = torchaudio.load(mp3_path)\n",
    "                    # Convert to mono if needed\n",
    "                    if waveform.shape[0] > 1:\n",
    "                        audio = waveform.mean(dim=0).numpy()\n",
    "                    else:\n",
    "                        audio = waveform.squeeze(0).numpy()\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # If torchaudio failed or file isn't MP3, try soundfile\n",
    "            if audio is None:\n",
    "                try:\n",
    "                    with sf.SoundFile(mp3_path) as sound_file:\n",
    "                        audio = sound_file.read(dtype='float32')\n",
    "                        orig_sr = sound_file.samplerate\n",
    "                        # Convert to mono if stereo\n",
    "                        if audio.ndim > 1:\n",
    "                            audio = audio.mean(axis=1)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Last resort: try librosa with explicit parameters to avoid deprecated functions\n",
    "            if audio is None:\n",
    "                try:\n",
    "                    # Updated librosa loading to avoid deprecated functions\n",
    "                    audio, orig_sr = librosa.load(\n",
    "                        mp3_path,\n",
    "                        sr=None,\n",
    "                        mono=True,\n",
    "                        duration=None,  # No duration limit\n",
    "                        offset=0.0,     # Start from beginning\n",
    "                        dtype=np.float32,\n",
    "                        res_type='kaiser_best'  # High quality resampling\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    return False, f\"{mp3_path}: Failed to load with librosa: {str(e)}\"\n",
    "        \n",
    "        # If we still don't have audio, return failure\n",
    "        if audio is None:\n",
    "            return False, f\"{mp3_path}: Failed to load audio with all methods\"\n",
    "            \n",
    "        # Resample if needed - updated to use current preferred approach\n",
    "        if orig_sr != sr:\n",
    "            audio = librosa.resample(\n",
    "                y=audio, \n",
    "                orig_sr=orig_sr, \n",
    "                target_sr=sr,\n",
    "                res_type='kaiser_best'\n",
    "            )\n",
    "        \n",
    "        # Make sure the directory exists\n",
    "        os.makedirs(os.path.dirname(wav_path), exist_ok=True)\n",
    "        \n",
    "        # Save as wav\n",
    "        sf.write(wav_path, audio, sr)\n",
    "        return True, mp3_path\n",
    "    except Exception as e:\n",
    "        return False, f\"{mp3_path}: {str(e)}\"\n",
    "\n",
    "def convert_dataset(in_dir, out_dir, n_workers=4):\n",
    "    \"\"\"Convert all MP3 files in the input directory to WAV files\"\"\"\n",
    "    # Get all mp3 files\n",
    "    mp3_files = []\n",
    "    \n",
    "    # Check if the root directory exists\n",
    "    if not os.path.exists(in_dir):\n",
    "        print(f\"Error: Input directory {in_dir} does not exist\")\n",
    "        return []\n",
    "        \n",
    "    print(f\"Searching for MP3 files in {in_dir}\")\n",
    "    \n",
    "    # List all files in the directory\n",
    "    try:\n",
    "        contents = os.listdir(in_dir)\n",
    "        print(f\"Contents of {in_dir}: {contents}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing directory contents: {e}\")\n",
    "        return []\n",
    "    \n",
    "    # Check each language directory\n",
    "    for lang in LANGS:\n",
    "        lang_dir = os.path.join(in_dir, lang)\n",
    "        if not os.path.exists(lang_dir):\n",
    "            print(f\"Warning: {lang_dir} does not exist, skipping\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Found language directory: {lang}\")\n",
    "        \n",
    "        # Walk through the language directory\n",
    "        for root, _, files in os.walk(lang_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.mp3'):\n",
    "                    mp3_path = os.path.join(root, file)\n",
    "                    # Get relative path to preserve directory structure\n",
    "                    rel_path = os.path.relpath(mp3_path, in_dir)\n",
    "                    # Replace extension\n",
    "                    wav_path = os.path.join(out_dir, rel_path.replace('.mp3', '.wav'))\n",
    "                    mp3_files.append((mp3_path, wav_path))\n",
    "    \n",
    "    print(f\"Found {len(mp3_files)} MP3 files to convert\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    # If no files found, return early\n",
    "    if len(mp3_files) == 0:\n",
    "        print(\"No MP3 files found to convert\")\n",
    "        return []\n",
    "    \n",
    "    # Convert files in parallel\n",
    "    results = []\n",
    "    with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "        futures = []\n",
    "        for mp3_path, wav_path in mp3_files:\n",
    "            futures.append(executor.submit(convert_mp3_to_wav, mp3_path, wav_path))\n",
    "        \n",
    "        # Monitor progress\n",
    "        processed = 0\n",
    "        total = len(futures)\n",
    "        print(f\"Converting {total} MP3 files to WAV...\")\n",
    "        for future in futures:\n",
    "            success, msg = future.result()\n",
    "            if not success:\n",
    "                results.append(msg)\n",
    "            processed += 1\n",
    "            if processed % 1000 == 0 or processed == total:\n",
    "                print(f\"Processed {processed}/{total} files ({processed/total*100:.1f}%)\")\n",
    "    \n",
    "    # Report errors\n",
    "    if results:\n",
    "        print(f\"{len(results)} files failed to convert:\")\n",
    "        for msg in results[:10]:  # Show only first 10 errors\n",
    "            print(f\"  - {msg}\")\n",
    "        if len(results) > 10:\n",
    "            print(f\"  - ... and {len(results) - 10} more\")\n",
    "    \n",
    "    print(f\"Successfully converted {len(mp3_files) - len(results)} out of {len(mp3_files)} files\")\n",
    "    return results\n",
    "\n",
    "def create_data_splits(data_dir, out_dir, split_ratio=(0.8, 0.1, 0.1), seed=42):\n",
    "    \"\"\"Create train/val/test splits from the WAV files\"\"\"\n",
    "    # Find all wav files and their languages\n",
    "    data = []\n",
    "    \n",
    "    # Check if data directory exists\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Error: Data directory {data_dir} does not exist\")\n",
    "        # Return empty DataFrames to avoid errors\n",
    "        empty_df = pd.DataFrame(columns=['path', 'lang'])\n",
    "        return empty_df, empty_df, empty_df\n",
    "    \n",
    "    for lang in LANGS:\n",
    "        lang_dir = os.path.join(data_dir, lang)\n",
    "        if not os.path.exists(lang_dir):\n",
    "            print(f\"Warning: {lang_dir} does not exist, skipping\")\n",
    "            continue\n",
    "            \n",
    "        for root, _, files in os.walk(lang_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.wav'):\n",
    "                    wav_path = os.path.join(root, file)\n",
    "                    data.append({'path': wav_path, 'lang': lang})\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    \n",
    "    # If no data found, return empty DataFrames\n",
    "    if len(df) == 0:\n",
    "        print(\"No WAV files found. Skipping data splits creation.\")\n",
    "        empty_df = pd.DataFrame(columns=['path', 'lang'])\n",
    "        return empty_df, empty_df, empty_df\n",
    "    \n",
    "    print(df['lang'].value_counts())\n",
    "    \n",
    "    # Shuffle data\n",
    "    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    \n",
    "    # Create splits for each language\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    for lang in LANGS:\n",
    "        lang_df = df[df['lang'] == lang]\n",
    "        if len(lang_df) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Calculate split sizes\n",
    "        n_train = int(len(lang_df) * split_ratio[0])\n",
    "        n_val = int(len(lang_df) * split_ratio[1])\n",
    "        \n",
    "        # Split the data\n",
    "        train_data.append(lang_df.iloc[:n_train])\n",
    "        val_data.append(lang_df.iloc[n_train:n_train+n_val])\n",
    "        test_data.append(lang_df.iloc[n_train+n_val:])\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    if train_data:\n",
    "        train_df = pd.concat(train_data).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    else:\n",
    "        train_df = pd.DataFrame(columns=['path', 'lang'])\n",
    "        \n",
    "    if val_data:\n",
    "        val_df = pd.concat(val_data).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    else:\n",
    "        val_df = pd.DataFrame(columns=['path', 'lang'])\n",
    "        \n",
    "    if test_data:\n",
    "        test_df = pd.concat(test_data).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    else:\n",
    "        test_df = pd.DataFrame(columns=['path', 'lang'])\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    # Save splits\n",
    "    train_df.to_csv(os.path.join(out_dir, 'train.csv'), index=False)\n",
    "    val_df.to_csv(os.path.join(out_dir, 'val.csv'), index=False)\n",
    "    test_df.to_csv(os.path.join(out_dir, 'test.csv'), index=False)\n",
    "    \n",
    "    print(f\"Splits created and saved to {out_dir}:\")\n",
    "    print(f\"  - Train: {len(train_df)} samples\")\n",
    "    print(f\"  - Validation: {len(val_df)} samples\")\n",
    "    print(f\"  - Test: {len(test_df)} samples\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# ================================================================\n",
    "# 8. MAIN TRAINING PIPELINE\n",
    "# ================================================================\n",
    "\n",
    "def train_model(train_csv, val_csv, test_csv=None, \n",
    "                whisper_size='base', freeze_encoder=True,\n",
    "                batch_size=32, epochs=5, lr=1e-4, weight_decay=1e-4,\n",
    "                patience=3, num_workers=4, device='cuda', output_dir='models'):\n",
    "    \"\"\"Full training pipeline\"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Check if training data is available\n",
    "    if not os.path.exists(train_csv) or os.path.getsize(train_csv) == 0:\n",
    "        print(f\"Error: Training data file {train_csv} does not exist or is empty\")\n",
    "        return None, None\n",
    "        \n",
    "    if not os.path.exists(val_csv) or os.path.getsize(val_csv) == 0:\n",
    "        print(f\"Error: Validation data file {val_csv} does not exist or is empty\")\n",
    "        return None, None\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(device)\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Get dataloaders\n",
    "    train_loader, val_loader, test_loader = get_dataloaders(\n",
    "        train_csv,\n",
    "        val_csv,\n",
    "        test_csv,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(\n",
    "        whisper_size=whisper_size,\n",
    "        num_languages=len(LANGS),\n",
    "        freeze_encoder=freeze_encoder,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Print number of trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model: Whisper-{whisper_size} for LID\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,} / {total_params:,}\")\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=epochs\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_f1 = 0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Track metrics\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    val_f1s = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        train_metrics = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        print(f\"Train Loss: {train_metrics['loss']:.4f}, Accuracy: {train_metrics['accuracy']:.4f}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "        print(f\"Val Loss: {val_metrics['loss']:.4f}, Accuracy: {val_metrics['accuracy']:.4f}, Macro F1: {val_metrics['macro_f1']:.4f}\")\n",
    "        \n",
    "        # Track metrics\n",
    "        train_losses.append(train_metrics['loss'])\n",
    "        train_accs.append(train_metrics['accuracy'])\n",
    "        val_losses.append(val_metrics['loss'])\n",
    "        val_accs.append(val_metrics['accuracy'])\n",
    "        val_f1s.append(val_metrics['macro_f1'])\n",
    "        \n",
    "        # Step scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save the model if it's the best so far\n",
    "        if val_metrics['macro_f1'] > best_f1:\n",
    "            best_f1 = val_metrics['macro_f1']\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save model\n",
    "            model_path = os.path.join(output_dir, f\"lid_whisper_{whisper_size}_epoch{epoch+1}_f1{best_f1:.4f}.pt\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"Saved best model to {model_path}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement for {patience_counter} epochs (best F1: {best_f1:.4f} at epoch {best_epoch+1})\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(train_losses, label='Train')\n",
    "    plt.plot(val_losses, label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss Curves')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(train_accs, label='Train')\n",
    "    plt.plot(val_accs, label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy Curves')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(val_f1s)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Macro F1')\n",
    "    plt.title('Validation F1 Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'training_curves.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Load best model for testing if a best model was saved\n",
    "    results = {}\n",
    "    if best_epoch >= 0:\n",
    "        best_model_path = os.path.join(output_dir, f\"lid_whisper_{whisper_size}_epoch{best_epoch+1}_f1{best_f1:.4f}.pt\")\n",
    "        if os.path.exists(best_model_path):\n",
    "            model.load_state_dict(torch.load(best_model_path))\n",
    "        \n",
    "            # Evaluate on test set\n",
    "            if test_loader is not None:\n",
    "                print(\"\\nEvaluating on test set...\")\n",
    "                test_metrics = evaluate(model, test_loader, criterion, device)\n",
    "                print(f\"Test Accuracy: {test_metrics['accuracy']:.4f}, Macro F1: {test_metrics['macro_f1']:.4f}\")\n",
    "                \n",
    "                # Plot confusion matrix\n",
    "                cm = confusion_matrix(test_metrics['labels'], test_metrics['preds'])\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=LANGS)\n",
    "                disp.plot(cmap='Blues', values_format='d', xticks_rotation=45)\n",
    "                plt.title(f'Confusion Matrix (Test Set)')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
    "                plt.show()\n",
    "                \n",
    "                # Save test results\n",
    "                results = {\n",
    "                    \"test_accuracy\": test_metrics['accuracy'],\n",
    "                    \"test_macro_f1\": test_metrics['macro_f1'],\n",
    "                    \"best_val_f1\": best_f1,\n",
    "                    \"best_epoch\": best_epoch + 1,\n",
    "                    \"model_path\": best_model_path,\n",
    "                    \"whisper_size\": whisper_size,\n",
    "                    \"freeze_encoder\": freeze_encoder,\n",
    "                }\n",
    "                \n",
    "                # Save results to JSON\n",
    "                with open(os.path.join(output_dir, \"results.json\"), \"w\") as f:\n",
    "                    json.dump(results, f, indent=2)\n",
    "    \n",
    "    return model, results\n",
    "\n",
    "# ================================================================\n",
    "# 9. INFERENCE FUNCTIONS\n",
    "# ================================================================\n",
    "\n",
    "def predict_language(model, audio_path, device='cuda'):\n",
    "    \"\"\"Predict language of an audio file\"\"\"\n",
    "    # Load audio with robust loader\n",
    "    audio, sr = None, None\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        \n",
    "        # Try soundfile first\n",
    "        try:\n",
    "            with sf.SoundFile(audio_path) as sound_file:\n",
    "                audio = sound_file.read(dtype='float32')\n",
    "                sr = sound_file.samplerate\n",
    "                if audio.ndim > 1:  # Convert to mono if stereo\n",
    "                    audio = audio.mean(axis=1)\n",
    "        except:\n",
    "            # Fall back to librosa\n",
    "            try:\n",
    "                audio, sr = librosa.load(\n",
    "                    audio_path,\n",
    "                    sr=None,\n",
    "                    mono=True,\n",
    "                    duration=None, \n",
    "                    offset=0.0,\n",
    "                    dtype=np.float32,\n",
    "                    res_type='kaiser_best'\n",
    "                )\n",
    "            except:\n",
    "                # Last resort: torchaudio\n",
    "                try:\n",
    "                    try:\n",
    "                        torchaudio.set_audio_backend(\"sox_io\")\n",
    "                    except:\n",
    "                        pass\n",
    "                    audio_tensor, sr = torchaudio.load(audio_path)\n",
    "                    if audio_tensor.shape[0] > 1:  # Multi-channel, convert to mono\n",
    "                        audio = audio_tensor.mean(dim=0).numpy()\n",
    "                    else:\n",
    "                        audio = audio_tensor.squeeze(0).numpy()\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to load {audio_path}: {e}\")\n",
    "                    sr = 16000\n",
    "                    audio = np.zeros(sr)\n",
    "    \n",
    "    audio = torch\n",
    "    audio = torch.tensor(audio).float()\n",
    "    \n",
    "    # Resample if needed\n",
    "    if sr != 16000:\n",
    "        audio = torchaudio.functional.resample(audio, orig_freq=sr, new_freq=16000)\n",
    "    \n",
    "    # Pad or trim\n",
    "    audio = whisper.audio.pad_or_trim(audio)\n",
    "    \n",
    "    # Extract mel spectrogram\n",
    "    mel = whisper.audio.log_mel_spectrogram(audio)\n",
    "    \n",
    "    # Add batch dimension and move to device\n",
    "    mel = mel.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Run inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(mel)\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "    \n",
    "    # Get top predictions\n",
    "    probs, indices = torch.topk(probabilities, 3)\n",
    "    \n",
    "    # Convert to languages\n",
    "    results = []\n",
    "    for i in range(3):\n",
    "        lang_idx = indices[0, i].item()\n",
    "        lang = ID2LANG[lang_idx]\n",
    "        prob = probs[0, i].item()\n",
    "        results.append((lang, prob))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def demo_inference(model, audio_path, device='cuda'):\n",
    "    \"\"\"Run inference and display results visually\"\"\"\n",
    "    # Get predictions\n",
    "    results = predict_language(model, audio_path, device=device)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Results for {os.path.basename(audio_path)}:\")\n",
    "    for lang, prob in results:\n",
    "        print(f\"  - {lang.capitalize()}: {prob*100:.2f}%\")\n",
    "    \n",
    "    # Plot probabilities as a bar chart\n",
    "    langs, probs = zip(*results)\n",
    "    langs = [l.capitalize() for l in langs]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(langs, probs, color='skyblue')\n",
    "    plt.xlabel('Language')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title(f'Language Prediction for {os.path.basename(audio_path)}')\n",
    "    plt.ylim(0, 1)\n",
    "    for i, prob in enumerate(probs):\n",
    "        plt.text(i, prob + 0.02, f'{prob*100:.1f}%', ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ================================================================\n",
    "# 10. EXECUTE THE PIPELINE\n",
    "# ================================================================\n",
    "\n",
    "# Debugging information about directories\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Checking if MP3_DIR exists: {os.path.exists(MP3_DIR)}\")\n",
    "if os.path.exists(MP3_DIR):\n",
    "    print(f\"Contents of MP3_DIR: {os.listdir(MP3_DIR)}\")\n",
    "else:\n",
    "    print(\"MP3_DIR does not exist or is not accessible\")\n",
    "    \n",
    "    # Try to find where the data might be\n",
    "    for root, dirs, files in os.walk(\"G44\"):\n",
    "        if \"Language Detection Dataset\" in dirs:\n",
    "            print(f\"Found 'Language Detection Dataset' in {root}\")\n",
    "            MP3_DIR = os.path.join(root, \"Language Detection Dataset\")\n",
    "            print(f\"Updated MP3_DIR to {MP3_DIR}\")\n",
    "            break\n",
    "\n",
    "# Check if the data is already processed\n",
    "wav_count = 0\n",
    "for lang in LANGS:\n",
    "    lang_dir = os.path.join(WAV_DIR, lang)\n",
    "    if os.path.exists(lang_dir):\n",
    "        for root, _, files in os.walk(lang_dir):\n",
    "            wav_count += sum(1 for f in files if f.endswith('.wav'))\n",
    "\n",
    "# Run preprocessing steps\n",
    "print(\"Starting data preprocessing...\")\n",
    "\n",
    "# Convert MP3 to WAV only if needed\n",
    "if wav_count < 1000:  # If we have fewer than 1000 WAV files, assume we need conversion\n",
    "    print(\"Converting MP3 files to WAV...\")\n",
    "    convert_dataset(MP3_DIR, WAV_DIR, n_workers=4)\n",
    "else:\n",
    "    print(f\"Skipping MP3 to WAV conversion, found {wav_count} existing WAV files\")\n",
    "\n",
    "# Create data splits\n",
    "print(\"Creating data splits...\")\n",
    "train_df, val_df, test_df = create_data_splits(WAV_DIR, SPLITS_DIR, split_ratio=(0.8, 0.1, 0.1))\n",
    "\n",
    "# Check if we have data to proceed\n",
    "if len(train_df) == 0 or len(val_df) == 0:\n",
    "    print(\"Insufficient data to train model. Please check your dataset paths.\")\n",
    "    print(\"You may need to manually download and organize the dataset before running this script.\")\n",
    "    print(f\"Expected MP3 files in: {MP3_DIR}\")\n",
    "    \n",
    "    # Instructions for manual setup\n",
    "    print(\"\\nSuggested steps to set up the dataset:\")\n",
    "    print(f\"1. Download the dataset from https://www.kaggle.com/datasets/hbchaitanyabharadwaj/audio-dataset-with-10-indian-languages\")\n",
    "    print(f\"2. Extract all files to: {MP3_DIR}\")\n",
    "    print(f\"3. Ensure the directory structure is: {MP3_DIR}/[Language]/[audio_files.mp3]\")\n",
    "    print(f\"4. Run this script again after setting up the dataset\")\n",
    "else:\n",
    "    # Train the model\n",
    "    print(\"Starting model training...\")\n",
    "    model, results = train_model(\n",
    "        train_csv=os.path.join(SPLITS_DIR, \"train.csv\"),\n",
    "        val_csv=os.path.join(SPLITS_DIR, \"val.csv\"),\n",
    "        test_csv=os.path.join(SPLITS_DIR, \"test.csv\"),\n",
    "        whisper_size=\"tiny\",  # Use tiny for faster training; options: tiny, base, small, medium, large\n",
    "        freeze_encoder=True,\n",
    "        batch_size=16,        # Reduced batch size for GPU memory\n",
    "        epochs=5,\n",
    "        lr=1e-4,\n",
    "        patience=3,\n",
    "        num_workers=2,        # Reduced workers for compatibility\n",
    "        device=DEVICE,\n",
    "        output_dir=OUTPUT_DIR\n",
    "    )\n",
    "\n",
    "    if model is not None:\n",
    "        print(\"Pipeline completed successfully!\")\n",
    "        \n",
    "        # Example of running inference if test files are available\n",
    "        test_files = []\n",
    "        if os.path.exists(os.path.join(SPLITS_DIR, \"test.csv\")):\n",
    "            test_df = pd.read_csv(os.path.join(SPLITS_DIR, \"test.csv\"))\n",
    "            if len(test_df) > 0:\n",
    "                # Get a few examples from each language for demo\n",
    "                test_files = []\n",
    "                for lang in LANGS:\n",
    "                    lang_files = test_df[test_df['lang'] == lang]['path'].values\n",
    "                    if len(lang_files) > 0:\n",
    "                        test_files.append(lang_files[0])  # Take first file of each language\n",
    "                        \n",
    "                # Run demo inference on sample files\n",
    "                if test_files:\n",
    "                    print(\"\\nDemonstrating inference on sample files:\")\n",
    "                    for audio_file in test_files[:3]:  # Limit to 3 examples\n",
    "                        if os.path.exists(audio_file):\n",
    "                            print(f\"\\nInference on {audio_file}:\")\n",
    "                            demo_inference(model, audio_file, device=DEVICE)\n",
    "    else:\n",
    "        print(\"Model training failed. Please check error messages.\")\n",
    "\n",
    "# Optional: Add a simple way to run inference on a specific file\n",
    "def run_inference_on_file(model_path, audio_path, device=DEVICE):\n",
    "    \"\"\"Run inference on a specific audio file using a trained model\"\"\"\n",
    "    # Load model\n",
    "    model = create_model(whisper_size=\"tiny\", num_languages=len(LANGS), device=device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    # Run demo inference\n",
    "    demo_inference(model, audio_path, device=device)\n",
    "\n",
    "# Example usage of the inference function:\n",
    "# model_path = os.path.join(OUTPUT_DIR, \"lid_whisper_tiny_epoch5_f10.8976.pt\")\n",
    "# audio_path = \"path/to/test/audio.wav\"\n",
    "# run_inference_on_file(model_path, audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac78bb9-c5fa-4f7a-aff8-676ccd5e8c61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
